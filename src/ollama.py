import json
import requests

async def generate_chat_response(prompt, temp_msg, context):
    result = ""
    try:
        result = await query_ollama(prompt)
        await context.bot.editMessageText(text=result, chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
        if not result:
            print("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏")
            await context.bot.editMessageText(text='üòî –Ø –Ω–µ –∑–Ω–∞—é —á—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å. –ü–æ–ø—Ä–æ–±—É–π —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ',
                                              chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
    except Exception as e:
        print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        await context.bot.editMessageText(text='üòî –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ',
                                          chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
    return result

async def query_ollama(prompt):
    response = requests.post('http://localhost:11434/api/generate', json={
        # 'model': 'qwen2:0.5b',
        'model': 'gemma2:2b',
        # 'model': 'llama3.1:latest',
        'prompt': prompt,
        'stream': True,
    })
    response.raise_for_status()
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–≤–µ—Ç –ø–æ—Å—Ç—Ä–æ—á–Ω–æ
    result = ""
    for line in response.iter_lines():
        if line:
            try:
                json_line = json.loads(line)
                if 'response' in json_line:
                    result += json_line['response']
            except json.JSONDecodeError:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —Å—Ç—Ä–æ–∫—É JSON: {line}")
    return result