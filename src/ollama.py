import json
import requests

async def generate_chat_response(prompt, temp_msg, context):
    result = ""
    try:
        result = await query_ollama(prompt)
        await context.bot.editMessageText(text=result, chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
        if not result:
            print("–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏")
            await context.bot.editMessageText(text='üòî –Ø –Ω–µ –∑–Ω–∞—é —á—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å. –ü–æ–ø—Ä–æ–±—É–π —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ',
                                              chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
    except Exception as e:
        print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        await context.bot.editMessageText(text='üòî –ß—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫. –ü–æ–ø—Ä–æ–±—É–π —á—Ç–æ-—Ç–æ –¥—Ä—É–≥–æ–µ',
                                          chat_id=temp_msg.chat_id, message_id=temp_msg.message_id)
    return result

async def query_ollama(prompt):
    response = requests.post('http://localhost:11434/api/generate', json={
        'model': 'wizard-vicuna-uncensored:13b',
        'prompt': prompt,
        'stream': False,
        'temperature': 0,
        'max_tokens': 182,
    })
    response.raise_for_status()
    
    result = ""
    json_response = response.json()
    if 'response' in json_response:
        result = json_response['response']
    else:
        print(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {json_response}")
    print(result)
    return result